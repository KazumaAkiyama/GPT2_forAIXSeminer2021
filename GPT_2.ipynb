{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT_2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0LEs17_wA4k"
      },
      "source": [
        "**メニュー「ランタイム→ランタイムのタイプを変更」**でハードウェアアクセラレータを**GPU**に変更して保存してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZWt9BJkZair",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88b08b6a-e20a-4882-f28e-e042493268e3"
      },
      "source": [
        "# GoogleDriveをマウントする\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#作業用フォルダの作成\n",
        "!mkdir -p '/content/drive/My Drive/AIX_seminner_2021/'\n",
        "\n",
        "#青空文庫データ保存用フォルダの作成\n",
        "!mkdir -p '/content/drive/My Drive/AIX_seminner_2021/aozora_data'\n",
        "\n",
        "# 学習済みモデル保存用フォルダの作成\n",
        "!mkdir -p '/content/drive/My Drive/AIX_seminner_2021/bert_data'\n",
        "\n",
        "#作業用フォルダに移動する\n",
        "%cd '/content/drive/My Drive/AIX_seminner_2021/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/AIX_seminner_2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P8dPYhArF_W"
      },
      "source": [
        "**マイドライブ＞AIX_seminner_2021>aozora_data** 内に青空文庫からダウンロードしたzipファイルを入れてください．\n",
        "[青空文庫](https://www.aozora.gr.jp/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beTSr6cSZ5ha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6451010f-0a67-4940-a497-0cb80b9f6586"
      },
      "source": [
        "# Huggingface Datasetsのインストール\n",
        "!pip install datasets==1.2.1\n",
        "\n",
        "# Sentencepieceのインストール\n",
        "!pip install sentencepiece==0.1.91"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets==1.2.1\n",
            "  Downloading datasets-1.2.1-py3-none-any.whl (159 kB)\n",
            "\u001b[?25l\r\u001b[K     |██                              | 10 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████                            | 20 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 30 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 40 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 51 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 61 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 71 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 81 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 102 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 112 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 122 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 133 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 143 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 153 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 159 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (0.70.12.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (1.19.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (0.3.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 46.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (3.0.0)\n",
            "Collecting tqdm<4.50.0,>=4.27\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (4.6.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.2.1) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.2.1) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.2.1) (1.15.0)\n",
            "Installing collected packages: xxhash, tqdm, datasets\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.0\n",
            "    Uninstalling tqdm-4.62.0:\n",
            "      Successfully uninstalled tqdm-4.62.0\n",
            "Successfully installed datasets-1.2.1 tqdm-4.49.0 xxhash-2.0.2\n",
            "Collecting sentencepiece==0.1.91\n",
            "  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 4.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuBnZFikaM-1",
        "outputId": "5ffa6b73-d965-41a0-9eb7-36760a622cfd"
      },
      "source": [
        "#google drive用ダウンロードツールのインストール\n",
        "!pip install gdown"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHiBuWFHZhTr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97dd2fb1-788e-43d7-ccc1-7ed37d3f010b"
      },
      "source": [
        "# ソースからのHuggingface Transformersのインストール\n",
        "!git clone https://github.com/huggingface/transformers -b v4.4.2\n",
        "!pip install -e transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n",
            "Obtaining file:///content/drive/My%20Drive/AIX_seminner_2021/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (4.49.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (4.6.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (3.0.12)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 38.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (21.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.4.2) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.4.2) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.4.2) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_SXiNpuwmF5"
      },
      "source": [
        "**メニュー「ランタイム → ランタイムを再起動」**で「Google Colab」を再起動"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbUMHmP-1fu7"
      },
      "source": [
        "※この作業がないとパッケージが反映されません"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBdrbH740it_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea86e0f7-8179-4243-de1b-ba8d36683c86"
      },
      "source": [
        "# 作業フォルダに戻る(ランタイムを再起動したので)\n",
        "%cd '/content/drive/My Drive/AIX_seminner_2021/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/AIX_seminner_2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZn7cRlbZ-Eu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dea50652-0317-4564-e7aa-37940018b941"
      },
      "source": [
        "#aozora_dataファルダに移動\n",
        "%cd '/content/drive/My Drive/AIX_seminner_2021/aozora_data'\n",
        "#フォルダ内の既存のtxtファイルをすべて削除\n",
        "!rm *.txt\n",
        "#フォルダ内のzipファイルを展開する\n",
        "!unzip '*.zip'\n",
        "#作業フォルダに移動\n",
        "%cd '/content/drive/My Drive/AIX_seminner_2021/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/AIX_seminner_2021/aozora_data\n",
            "Archive:  866_ruby_23838.zip\n",
            "  inflating: aru_uchujinno__himitsu.txt  \n",
            "\n",
            "Archive:  52192_ruby_46179.zip\n",
            "Made with MacWinZipper™\n",
            "  inflating: omoidasu_mamani.txt     \n",
            "\n",
            "Archive:  45973_ruby_38302.zip\n",
            "  inflating: nihonno_shinno_sugata.txt  \n",
            "\n",
            "3 archives were successfully processed.\n",
            "/content/drive/My Drive/AIX_seminner_2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6_YZ4eWaKcP"
      },
      "source": [
        "import re\n",
        "import glob\n",
        "\n",
        "#ファインチューニング用のデータを作成する\n",
        "train_text_list = []\n",
        "files = glob.glob('./aozora_data/*.txt')\n",
        "for file in files:\n",
        "  with open(file, encoding=\"shift-jis\") as f:\n",
        "    #スクレイピング処理\n",
        "    #最終的にtextは句点区切りの文を要素としてもつリストになる\n",
        "    text = f.read()\n",
        "    text = re.split('-{55}',text)\n",
        "    text = re.split('底本：',text[2])\n",
        "    text = re.sub('《.*》','',text[0])\n",
        "    text = re.sub('［＃.*］','', text)\n",
        "    text = re.split(\"(?<=。)\",text)\n",
        "  #テキストを一文ずつ分割したデータを作成\n",
        "  for sentence in text[0:-1]:\n",
        "    if len(sentence):\n",
        "      train_text_list.append(sentence.strip().replace('\\n',''))\n",
        "\n",
        "#データをtrain.txtとして保存\n",
        "with open(\"train.txt\", mode='w') as f:\n",
        "  f.write('\\n'.join(train_text_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYrs6yHdaQeA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7be2b71e-5045-433b-aecb-0360a1c81254"
      },
      "source": [
        "#Huggingfaceのファインチューニング実行用ファイルのダウンロード\n",
        "!gdown https://drive.google.com/uc?id=1I9PmXXzLkqaILJLvlq3g6wtVfi8_Z2a1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1I9PmXXzLkqaILJLvlq3g6wtVfi8_Z2a1\n",
            "To: /content/drive/My Drive/AIX_seminner_2021/run_clm.py\n",
            "\r  0% 0.00/18.7k [00:00<?, ?B/s]\r100% 18.7k/18.7k [00:00<00:00, 5.64MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxjod_AYaUH6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d35ce3-46dc-48fe-d651-bfb995d16ae9"
      },
      "source": [
        "%%time\n",
        "\n",
        "!rm -r ./output\n",
        "# ファインチューニングの実行\n",
        "!python ./run_clm.py \\\n",
        "    --model_name_or_path=rinna/japanese-gpt2-medium \\\n",
        "    --train_file=train.txt \\\n",
        "    --validation_file=train.txt \\\n",
        "    --do_train \\\n",
        "\n",
        "    \n",
        "    --do_eval \\\n",
        "    --num_train_epochs=3 \\\n",
        "    --save_steps=5000 \\\n",
        "    --save_total_limit=3 \\\n",
        "    --per_device_train_batch_size=1 \\\n",
        "    --per_device_eval_batch_size=1 \\\n",
        "    --output_dir=output/ \\\n",
        "    --use_fast_tokenizer=False \\\n",
        "    --block_size 512"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove './output': No such file or directory\n",
            "08/29/2021 07:21:53 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/29/2021 07:21:53 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=output/, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Aug29_07-21-53_bfce8f9c01ba, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=5000, save_total_limit=3, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=output/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1)\n",
            "Downloading: 2.57kB [00:00, 1.28MB/s]       \n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset text/default-e516439b72de8851 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-e516439b72de8851/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-e516439b72de8851/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1386] 2021-08-29 07:21:54,576 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpg_sxrtat\n",
            "Downloading: 100% 799/799 [00:00<00:00, 578kB/s]\n",
            "[INFO|file_utils.py:1390] 2021-08-29 07:21:54,715 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.52b718e4a0d99e40aaaf447d3818b20b306909860ff4d3623cf948c2cc5c7570\n",
            "[INFO|file_utils.py:1393] 2021-08-29 07:21:54,715 >> creating metadata file for /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.52b718e4a0d99e40aaaf447d3818b20b306909860ff4d3623cf948c2cc5c7570\n",
            "[INFO|configuration_utils.py:463] 2021-08-29 07:21:54,715 >> loading configuration file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.52b718e4a0d99e40aaaf447d3818b20b306909860ff4d3623cf948c2cc5c7570\n",
            "[INFO|configuration_utils.py:499] 2021-08-29 07:21:54,716 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": 4096,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1386] 2021-08-29 07:21:54,848 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmplk0e1mwl\n",
            "Downloading: 100% 806k/806k [00:00<00:00, 15.4MB/s]\n",
            "[INFO|file_utils.py:1390] 2021-08-29 07:21:54,999 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|file_utils.py:1393] 2021-08-29 07:21:54,999 >> creating metadata file for /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|file_utils.py:1386] 2021-08-29 07:21:55,266 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6ss31mnd\n",
            "Downloading: 100% 153/153 [00:00<00:00, 121kB/s]\n",
            "[INFO|file_utils.py:1390] 2021-08-29 07:21:55,402 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|file_utils.py:1393] 2021-08-29 07:21:55,402 >> creating metadata file for /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|file_utils.py:1386] 2021-08-29 07:21:55,526 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp96i3o47v\n",
            "Downloading: 100% 282/282 [00:00<00:00, 214kB/s]\n",
            "[INFO|file_utils.py:1390] 2021-08-29 07:21:55,655 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.9c6d86638d0c8b0d39297c982899c13374e883f6e85a7c2c9baad32a40abf7dd\n",
            "[INFO|file_utils.py:1393] 2021-08-29 07:21:55,655 >> creating metadata file for /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.9c6d86638d0c8b0d39297c982899c13374e883f6e85a7c2c9baad32a40abf7dd\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-29 07:21:55,784 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-29 07:21:55,784 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-29 07:21:55,784 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-29 07:21:55,784 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.9c6d86638d0c8b0d39297c982899c13374e883f6e85a7c2c9baad32a40abf7dd\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-29 07:21:55,784 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|file_utils.py:1386] 2021-08-29 07:21:55,978 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvlad3cpu\n",
            "Downloading: 100% 1.37G/1.37G [00:37<00:00, 36.3MB/s]\n",
            "[INFO|file_utils.py:1390] 2021-08-29 07:22:33,773 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.cc70e8c279faa70b5303f7802493074be521ae42d129355a708b119ca945c8cb\n",
            "[INFO|file_utils.py:1393] 2021-08-29 07:22:33,773 >> creating metadata file for /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.cc70e8c279faa70b5303f7802493074be521ae42d129355a708b119ca945c8cb\n",
            "[INFO|modeling_utils.py:1051] 2021-08-29 07:22:33,773 >> loading weights file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.cc70e8c279faa70b5303f7802493074be521ae42d129355a708b119ca945c8cb\n",
            "[INFO|modeling_utils.py:1167] 2021-08-29 07:22:44,137 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1176] 2021-08-29 07:22:44,137 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at rinna/japanese-gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "100% 1/1 [00:00<00:00, 12.13ba/s]\n",
            "100% 1/1 [00:00<00:00, 10.84ba/s]\n",
            "100% 1/1 [00:00<00:00, 14.19ba/s]\n",
            "100% 1/1 [00:00<00:00, 15.41ba/s]\n",
            "[INFO|trainer.py:946] 2021-08-29 07:22:54,755 >> ***** Running training *****\n",
            "[INFO|trainer.py:947] 2021-08-29 07:22:54,756 >>   Num examples = 38\n",
            "[INFO|trainer.py:948] 2021-08-29 07:22:54,756 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:949] 2021-08-29 07:22:54,756 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:950] 2021-08-29 07:22:54,756 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:951] 2021-08-29 07:22:54,756 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:952] 2021-08-29 07:22:54,756 >>   Total optimization steps = 114\n",
            "100% 114/114 [01:56<00:00,  1.03s/it][INFO|trainer.py:1129] 2021-08-29 07:24:51,693 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 116.9367, 'train_samples_per_second': 0.975, 'epoch': 3.0}\n",
            "100% 114/114 [01:56<00:00,  1.03s/it]\n",
            "[INFO|trainer.py:1558] 2021-08-29 07:24:52,081 >> Saving model checkpoint to output/\n",
            "[INFO|configuration_utils.py:314] 2021-08-29 07:24:52,086 >> Configuration saved in output/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-08-29 07:24:58,537 >> Model weights saved in output/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-08-29 07:24:58,542 >> tokenizer config file saved in output/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-08-29 07:24:58,547 >> Special tokens file saved in output/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-08-29 07:24:58,556 >> Copy vocab file to output/spiece.model\n",
            "[INFO|trainer_pt_utils.py:656] 2021-08-29 07:24:58,565 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:24:58,565 >>   epoch                      =      3.0\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:24:58,565 >>   init_mem_cpu_alloc_delta   =      1MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:24:58,565 >>   init_mem_cpu_peaked_delta  =      0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:24:58,565 >>   init_mem_gpu_alloc_delta   =   1307MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:24:58,565 >>   init_mem_gpu_peaked_delta  =      0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:24:58,565 >>   train_mem_cpu_alloc_delta  =      0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:24:58,566 >>   train_mem_cpu_peaked_delta =      0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:24:58,566 >>   train_mem_gpu_alloc_delta  =   3849MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:24:58,566 >>   train_mem_gpu_peaked_delta =   2876MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:24:58,566 >>   train_runtime              = 116.9367\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:24:58,566 >>   train_samples              =       38\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:24:58,566 >>   train_samples_per_second   =    0.975\n",
            "08/29/2021 07:24:58 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1775] 2021-08-29 07:24:58,695 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1776] 2021-08-29 07:24:58,695 >>   Num examples = 38\n",
            "[INFO|trainer.py:1777] 2021-08-29 07:24:58,696 >>   Batch size = 1\n",
            "100% 38/38 [00:10<00:00,  3.71it/s]\n",
            "[INFO|trainer_pt_utils.py:656] 2021-08-29 07:25:09,169 >> ***** eval metrics *****\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:25:09,170 >>   epoch                     =     3.0\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:25:09,170 >>   eval_loss                 =  2.6182\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:25:09,170 >>   eval_mem_cpu_alloc_delta  =     0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:25:09,170 >>   eval_mem_cpu_peaked_delta =     0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:25:09,171 >>   eval_mem_gpu_alloc_delta  =     0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:25:09,171 >>   eval_mem_gpu_peaked_delta =   270MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:25:09,171 >>   eval_runtime              = 10.2723\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:25:09,171 >>   eval_samples              =      38\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:25:09,171 >>   eval_samples_per_second   =   3.699\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-29 07:25:09,172 >>   perplexity                = 13.7105\n",
            "CPU times: user 2.3 s, sys: 384 ms, total: 2.69 s\n",
            "Wall time: 3min 31s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpscAmQ3sI3b"
      },
      "source": [
        "**GPT2のみ用いた推論**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pdygRzqTPDjB",
        "outputId": "89ccb80a-bb22-4f6d-e8ae-6af53756b4c9"
      },
      "source": [
        "from transformers import T5Tokenizer, AutoModelForCausalLM\n",
        "import re\n",
        "\n",
        "#ファインチューニングしたモデルを用いる\n",
        "#USE_FINETUNED_GPT2 = True\n",
        "#ファインチューニングしたモデルを用いない\n",
        "USE_FINETUNED_GPT2 = False\n",
        "#候補文をいくつ表示するか\n",
        "OPTION_NUM = 4\n",
        "\n",
        "# トークナイザーの準備\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "#モデルの準備\n",
        "if USE_FINETUNED_GPT2:\n",
        "  model = AutoModelForCausalLM.from_pretrained(\"output/\")\n",
        "else:\n",
        "  model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "\n",
        "u = \"\"\n",
        "next_sentence = input(\"\\n>\")\n",
        "log = []\n",
        "log.append(next_sentence)\n",
        "\n",
        "while(True):  \n",
        "  if \"exit\" == u:\n",
        "    break\n",
        "  if \"back\" == u:\n",
        "    log.pop()\n",
        "    next_sentence = re.split(\"(?<=。)\",log[-1])[0]\n",
        "    print(\"入力文：\")\n",
        "    print(next_sentence)\n",
        "  if \"log\" == u:\n",
        "    print(\"ログ：\")\n",
        "    print(\"\\n\")\n",
        "    print(\"\\n\".join(log))\n",
        "    print(\"\\n\")\n",
        "    print(\"入力文：\")\n",
        "    print(next_sentence)\n",
        "  if not \"。\" in next_sentence:\n",
        "    next_sentence = next_sentence + \"。\" \n",
        "  # 推論\n",
        "  encoded = tokenizer.encode(next_sentence, return_tensors=\"pt\")\n",
        "  output = model.generate(encoded, do_sample=True, max_length=100, num_return_sequences=OPTION_NUM)\n",
        "\n",
        "  sequence_list = []\n",
        "  for sequence in tokenizer.batch_decode(output):\n",
        "    sequence = sequence.replace('</s>', '')\n",
        "    sentence_list = re.split(\"(?<=。)\",sequence)[:-1]\n",
        "    sequence = \"\".join(sentence_list)\n",
        "    sequence_list.append(sequence)\n",
        "  for i,sequence in enumerate(sequence_list):\n",
        "    print(\"[\", i,\"]\",sequence)\n",
        "  \n",
        "  u = input(\"\\n>\")\n",
        "  if u.isdecimal():\n",
        "    choice_sequence = sequence_list[int(u)]\n",
        "    log.append(choice_sequence[len(next_sentence):])\n",
        "    next_sentence = re.split(\"(?<=。)\", choice_sequence)[-2]\n",
        "    print(\"入力文：\")\n",
        "    print(next_sentence)\n",
        "\n",
        "  # 1. \">\"の右の入力欄に最初の一文を入力します\n",
        "  # 2. 入力文に続く[0]～[3]までの候補文が表示されます\n",
        "  # 3. 表示された候補の左の数字を\">\"の右の入力欄に入力することでその候補の最後の文が次の入力文になります\n",
        "  # 4. \"log\"と入力するとこれまでの文章が続けて表示されます\n",
        "  # 5. \"back\"と入力すると入力が一つ前まで戻ります\n",
        "  # 6. \"exit\"と入力すると終了します"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-5-5667b92227f3>\", line 1, in <module>\n",
            "    from transformers import T5Tokenizer, AutoModelForCausalLM\n",
            "  File \"<frozen importlib._bootstrap>\", line 1032, in _handle_fromlist\n",
            "  File \"/content/drive/My Drive/AIX_seminner_2021/transformers/src/transformers/__init__.py\", line 2310, in __getattr__\n",
            "    return super().__getattr__(name)\n",
            "  File \"/content/drive/My Drive/AIX_seminner_2021/transformers/src/transformers/file_utils.py\", line 1660, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "  File \"/content/drive/My Drive/AIX_seminner_2021/transformers/src/transformers/__init__.py\", line 2304, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/content/drive/My Drive/AIX_seminner_2021/transformers/src/transformers/models/__init__.py\", line 19, in <module>\n",
            "    from . import (\n",
            "  File \"/content/drive/My Drive/AIX_seminner_2021/transformers/src/transformers/models/mt5/__init__.py\", line 36, in <module>\n",
            "    from ..t5.tokenization_t5_fast import T5TokenizerFast\n",
            "  File \"/content/drive/My Drive/AIX_seminner_2021/transformers/src/transformers/models/t5/tokenization_t5_fast.py\", line 23, in <module>\n",
            "    from ...tokenization_utils_fast import PreTrainedTokenizerFast\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 818, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 917, in get_data\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1464, in getframeinfo\n",
            "    lines, lnum = findsource(frame)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 182, in findsource\n",
            "    lines = linecache.getlines(file, globals_dict)\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 47, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 136, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 449, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 418, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 376, in read_or_stop\n",
            "    return readline()\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReW2XN6orDxa"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "**ここからBERTの学習とそれを活用した推論**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvrB6gXYcfHd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cdc2fb4-1684-4500-f9b4-f8acf6948fb9"
      },
      "source": [
        "#ライブラリのインストール\n",
        "!apt install git make curl xz-utils file\n",
        "!apt install mecab libmecab-dev mecab-ipadic mecab-ipadic-utf8\n",
        "!pip install mecab-python3==0.996.5\n",
        "!pip install fugashi\n",
        "!pip install ipadic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "make is already the newest version (4.1-9.1ubuntu1).\n",
            "make set to manually installed.\n",
            "xz-utils is already the newest version (5.2.2-1.3).\n",
            "xz-utils set to manually installed.\n",
            "curl is already the newest version (7.58.0-2ubuntu3.14).\n",
            "git is already the newest version (1:2.17.1-1ubuntu0.8).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libmagic-mgc libmagic1\n",
            "The following NEW packages will be installed:\n",
            "  file libmagic-mgc libmagic1\n",
            "0 upgraded, 3 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 275 kB of archives.\n",
            "After this operation, 5,297 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.4 [22.1 kB]\n",
            "Fetched 275 kB in 2s (164 kB/s)\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "(Reading database ... 148489 files and directories currently installed.)\n",
            "Preparing to unpack .../libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package file.\n",
            "Preparing to unpack .../file_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking file (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up file (1:5.32-2ubuntu0.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libmecab2 mecab-utils\n",
            "The following NEW packages will be installed:\n",
            "  libmecab-dev libmecab2 mecab mecab-ipadic mecab-ipadic-utf8 mecab-utils\n",
            "0 upgraded, 6 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 12.8 MB of archives.\n",
            "After this operation, 60.4 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab2 amd64 0.996-5 [257 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab-dev amd64 0.996-5 [308 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-utils amd64 0.996-5 [4,856 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic all 2.7.0-20070801+main-1 [12.1 MB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab amd64 0.996-5 [132 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-1 [3,522 B]\n",
            "Fetched 12.8 MB in 4s (3,545 kB/s)\n",
            "Selecting previously unselected package libmecab2:amd64.\n",
            "(Reading database ... 148521 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libmecab2_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab2:amd64 (0.996-5) ...\n",
            "Selecting previously unselected package libmecab-dev.\n",
            "Preparing to unpack .../1-libmecab-dev_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab-dev (0.996-5) ...\n",
            "Selecting previously unselected package mecab-utils.\n",
            "Preparing to unpack .../2-mecab-utils_0.996-5_amd64.deb ...\n",
            "Unpacking mecab-utils (0.996-5) ...\n",
            "Selecting previously unselected package mecab-ipadic.\n",
            "Preparing to unpack .../3-mecab-ipadic_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Selecting previously unselected package mecab.\n",
            "Preparing to unpack .../4-mecab_0.996-5_amd64.deb ...\n",
            "Unpacking mecab (0.996-5) ...\n",
            "Selecting previously unselected package mecab-ipadic-utf8.\n",
            "Preparing to unpack .../5-mecab-ipadic-utf8_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Setting up libmecab2:amd64 (0.996-5) ...\n",
            "Setting up mecab-utils (0.996-5) ...\n",
            "Setting up mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up libmecab-dev (0.996-5) ...\n",
            "Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab (0.996-5) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting mecab-python3==0.996.5\n",
            "  Downloading mecab_python3-0.996.5-cp37-cp37m-manylinux2010_x86_64.whl (17.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.1 MB 69 kB/s \n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-0.996.5\n",
            "Collecting fugashi\n",
            "  Downloading fugashi-1.1.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (490 kB)\n",
            "\u001b[K     |████████████████████████████████| 490 kB 4.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: fugashi\n",
            "Successfully installed fugashi-1.1.1\n",
            "Collecting ipadic\n",
            "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.4 MB 198 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: ipadic\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556723 sha256=12cc393642094ad1cfa3c481b46310e8dd935f4ea69cb3461dae252b13b201f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/8b/99/cf0d27191876637cd3639a560f93aa982d7855ce826c94348b\n",
            "Successfully built ipadic\n",
            "Installing collected packages: ipadic\n",
            "Successfully installed ipadic-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikU7fNEDc-4g"
      },
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "#aozora_dataフォルダ内のtxtファイルをつかってpairデータを作成する\n",
        "pair_list = []\n",
        "files = glob.glob('./aozora_data/*.txt')\n",
        "for file in files:\n",
        "  with open(file, encoding=\"shift-jis\") as f:\n",
        "    #スクレイピング処理\n",
        "    #最終的にtextは句点区切りの文を要素としてもつリストになる\n",
        "    text = f.read()\n",
        "    text = re.split('-{55}',text)\n",
        "    text = re.split('底本：',text[2])\n",
        "    text = re.sub('《.*》','',text[0])\n",
        "    text = re.sub('［＃.*］','', text)\n",
        "    text = re.split(\"(?<=。)\",text)\n",
        "    #1～3文と1～3文が対応するペアデータを作成\n",
        "    for i in range(len(text)-6):\n",
        "      m = random.randint(1,3)\n",
        "      n = random.randint(1,3)\n",
        "      pair_list.append(\"\".join(text[i:i+m])+\",\"+\"\".join(text[i+m:i+m+n]))\n",
        "\n",
        "#ペアデータをpair.txtとして保存\n",
        "with open(\"pair.txt\", mode='w') as f:\n",
        "  f.write('\\n'.join(pair_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc2unyvApHhN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30367ecc-6304-4db2-fa5c-40609b9b8f66"
      },
      "source": [
        "#BERTの学習用ファイルのダウンロード\n",
        "!gdown https://drive.google.com/uc?id=1VOsR57Zoyy97lxDMGF1euTzNQJpyoZ7E"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VOsR57Zoyy97lxDMGF1euTzNQJpyoZ7E\n",
            "To: /content/drive/My Drive/AIX_seminner_2021/run_glue.py\n",
            "\r  0% 0.00/23.4k [00:00<?, ?B/s]\r100% 23.4k/23.4k [00:00<00:00, 8.79MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p1H7AxOdCDO"
      },
      "source": [
        "write_lines = []\n",
        "uttrs = []\n",
        "\n",
        "filename = 'pair.txt'\n",
        "\n",
        "with open(filename) as f:\n",
        "    for l in f:\n",
        "        l = l.strip()\n",
        "        if \",\" in l:\n",
        "            # 実際の応答ペアを正解とし，ラベルは1とする．\n",
        "            write_lines.append(\"1,\" + l.split(\",\")[0] + l.split(\",\")[1] + \"\\n\")\n",
        "            # 不正解ペアの作成のため，発話を保存\n",
        "            uttrs.append(l.split(\",\")[0])\n",
        "            uttrs.append(l.split(\",\")[1])\n",
        "  \n",
        "# 正解ペアと同じ数だけ不正解ペアを作成\n",
        "for i in range(len(write_lines)):\n",
        "    # ランダムな応答ペアを不正解とし，ラベルは0とする．\n",
        "    write_lines.append(\"0,\" + random.choice(uttrs) + random.choice(uttrs) +  \"\\n\")\n",
        "  \n",
        " # 正解ペアと不正解ペアが入ったリストをシャッフルする\n",
        "random.shuffle(write_lines)\n",
        "  \n",
        "index = 0\n",
        "with open(\"bert_data/dev.csv\", \"w\") as var_f:\n",
        "    # 開発データとしてdev.tsvに200行を書き込む．\n",
        "    var_f.write(\"label,sentence\\n\")\n",
        "    for l in write_lines[:200]:\n",
        "        var_f.write(l)\n",
        "        index += 1\n",
        "index = 0\n",
        "with open(\"bert_data/train.csv\", \"w\") as var_f:\n",
        "    # 学習データとしてtrain.tsvにのこりを書き込む．\n",
        "    var_f.write(\"label,sentence\\n\")\n",
        "    for l in write_lines[200:]:\n",
        "        var_f.write(l)\n",
        "        index += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBubSjUadMCI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87688e1f-8361-4b67-a554-690d43743ff8"
      },
      "source": [
        "# max_stepsの値を大きな値に設定することで，より多くのデータで学習できるが，より多くの時間が必要となる\n",
        "!python transformers/examples/text-classification/run_glue.py --overwrite_output_dir \\\n",
        "--model_name_or_path cl-tohoku/bert-base-japanese-whole-word-masking --save_steps 1000 --max_steps 1000 \\\n",
        "--output_dir bert_output/ --do_train --do_eval --per_gpu_train_batch_size 16 --train_file bert_data/train.csv --validation_file bert_data/dev.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "08/31/2021 04:35:18 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/31/2021 04:35:18 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=bert_output/, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=1000, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Aug31_04-35-18_3aa6ba4f8304, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=1000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=bert_output/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1)\n",
            "08/31/2021 04:35:18 - INFO - __main__ -   load a local file for train: bert_data/train.csv\n",
            "08/31/2021 04:35:18 - INFO - __main__ -   load a local file for validation: bert_data/dev.csv\n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset csv/default-e228710fd8e8842e (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-e228710fd8e8842e/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2...\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-e228710fd8e8842e/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:463] 2021-08-31 04:35:21,775 >> loading configuration file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/573af37b6c39d672f2df687c06ad7d556476cbe43e5bf7771097187c45a3e7bf.abeb707b5d79387dd462e8bfb724637d856e98434b6931c769b8716c6f287258\n",
            "[INFO|configuration_utils.py:499] 2021-08-31 04:35:21,775 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
            "  \"transformers_version\": \"4.4.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:463] 2021-08-31 04:35:22,529 >> loading configuration file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/573af37b6c39d672f2df687c06ad7d556476cbe43e5bf7771097187c45a3e7bf.abeb707b5d79387dd462e8bfb724637d856e98434b6931c769b8716c6f287258\n",
            "[INFO|configuration_utils.py:499] 2021-08-31 04:35:22,529 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
            "  \"transformers_version\": \"4.4.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-31 04:35:26,256 >> loading file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/15164357d71cd32532e56c1d7c2757141326ae17c53e2277bc417cc7c21da6ea.a7378a0cbee5cff668832a776d72b97a25479604fe9564d5595897f75049e7f4\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-31 04:35:26,256 >> loading file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-31 04:35:26,256 >> loading file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-31 04:35:26,256 >> loading file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/0e46f722799f19c3f0c53172545108a4b31847d3b9a2d5b100759f6673bd667b.08ae4e4044742b9cc7172698caf1da2524f5597ff8cf848114dd0b730cc44bdc\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-31 04:35:26,257 >> loading file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|modeling_utils.py:1051] 2021-08-31 04:35:27,064 >> loading weights file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/cabd9bbd81093f4c494a02e34eb57e405b7564db216404108c8e8caf10ede4fa.464b54997e35e3cc3223ba6d7f0abdaeb7be5b7648f275f57d839ee0f95611fb\n",
            "[WARNING|modeling_utils.py:1159] 2021-08-31 04:35:30,520 >> Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1170] 2021-08-31 04:35:30,520 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 1/1 [00:00<00:00,  1.58ba/s]\n",
            "100% 1/1 [00:00<00:00,  7.12ba/s]\n",
            "08/31/2021 04:35:31 - INFO - __main__ -   Sample 654 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], 'input_ids': [2, 27014, 5537, 2259, 767, 144, 5, 12886, 7, 9, 11872, 5, 1473, 14, 25265, 16, 322, 10, 8, 1948, 18, 6983, 1713, 14, 3918, 3918, 13, 115, 7796, 16, 322, 10, 8, 324, 17005, 5, 4671, 9, 4492, 28449, 45, 28, 6303, 45, 28, 7767, 16, 6, 5537, 616, 5, 188, 29016, 144, 5, 174, 7, 5394, 6, 70, 174, 28571, 29310, 29243, 5, 212, 28637, 28652, 7, 18780, 28506, 1281, 10, 8, 17356, 28449, 28446, 91, 9, 6419, 7, 755, 989, 714, 19, 12, 8821, 91, 11, 1, 118, 10, 8, 9962, 75, 6, 25188, 12, 31, 6, 8541, 9, 1704, 13225, 1259, 1, 12, 3502, 28489, 29, 6, 19497, 45, 9, 1, 5, 749, 75, 13, 13225, 1259, 1, 12, 31, 8, 3, 0, 0], 'label': 0, 'sentence': 'いよいよ渋谷博士愛機の視野には火星の姿が映ってきた。有名な運河帯がアリアリと現われてきた。世界じゅうの人類は寝ることも食べることも忘れて、渋谷式の受影機の前に並び、この前代未聞の見世物にながめいった。然るに日本は僅に四五十年で近世日本を拵へた。不思議だ、ミラクルである、是はどう云ふ譯であらうか、此ことは餘程の問題だと云ふ譯である。', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/31/2021 04:35:31 - INFO - __main__ -   Sample 114 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], 'input_ids': [2, 218, 9, 52, 640, 962, 4590, 6, 1325, 5, 319, 29389, 7, 5, 28482, 28468, 16, 33, 97, 287, 6833, 40, 4587, 186, 5, 5537, 2259, 7, 1587, 9249, 28549, 15, 6, 454, 28611, 5, 7380, 15240, 13, 12768, 6670, 5, 7426, 13, 11, 319, 7, 8326, 1058, 14, 82, 12, 102, 10, 8, 893, 59, 288, 7, 9, 52, 29630, 28, 4590, 6, 9072, 5, 13459, 144, 11, 17461, 16, 9606, 5, 333, 600, 40, 333, 15, 1549, 82, 962, 102, 10, 8, 14072, 5755, 3181, 7, 1517, 13, 6, 11872, 2886, 126, 6, 1978, 6, 1035, 14619, 28, 284, 14, 31, 947, 6, 13162, 1225, 29103, 7, 2950, 28461, 312, 42, 30340, 29219, 12632, 28474, 28489, 120, 13, 2649, 20, 10, 8, 3, 0, 0], 'label': 1, 'sentence': 'それは一時間でも早く、私の手許にのこっている第二号機からロケット内の渋谷博士にインタービュウし、空前の探検譚と処女航路の風景とを手にいれんがためであった。そしてその次には一刻も早く、同型のテレビジョン機をつくって自国の放送局から放送したいためでもあった。なにしろ計算によると、火星到着まで、七、八カ月も間があるので、これから至急につくれば大丈夫間にあうものと思われた。', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "08/31/2021 04:35:31 - INFO - __main__ -   Sample 25 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [2, 13818, 29249, 809, 1080, 9, 91, 5, 283, 14, 1, 3299, 12, 1830, 181, 10, 5, 2992, 8, 8541, 9, 2045, 1, 993, 5, 45, 2992, 14, 1, 11, 26733, 13, 13225, 1259, 28482, 13, 9, 13986, 11, 22609, 1043, 14, 31, 13, 13225, 1259, 45, 12, 31, 8, 13818, 378, 9, 2666, 13986, 2992, 8, 13818, 29703, 13986, 11, 3270, 2428, 16, 412, 28652, 11, 26733, 13, 13225, 1259, 1043, 14, 15037, 10, 114, 29333, 45, 18, 5, 2992, 8, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence': '其農業状態は日本の方が餘程進んで居つたのです。是は唯鍬一つのことですが鍬を造ると云ふことは鐵を採る技術があると云ふことである。其多くは砂鐵です。其砂鐵を溶かして金物を造ると云ふ技術があつたといふことなのです。', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:369] 2021-08-31 04:35:34,415 >> max_steps is given, it will override any value given in num_train_epochs\n",
            "[INFO|trainer.py:483] 2021-08-31 04:35:34,416 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence.\n",
            "[INFO|trainer.py:483] 2021-08-31 04:35:34,417 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence.\n",
            "[WARNING|training_args.py:607] 2021-08-31 04:35:34,648 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:607] 2021-08-31 04:35:34,654 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:946] 2021-08-31 04:35:34,655 >> ***** Running training *****\n",
            "[INFO|trainer.py:947] 2021-08-31 04:35:34,655 >>   Num examples = 933\n",
            "[INFO|trainer.py:948] 2021-08-31 04:35:34,655 >>   Num Epochs = 17\n",
            "[INFO|trainer.py:949] 2021-08-31 04:35:34,655 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:950] 2021-08-31 04:35:34,655 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:951] 2021-08-31 04:35:34,655 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:952] 2021-08-31 04:35:34,655 >>   Total optimization steps = 1000\n",
            "[WARNING|training_args.py:607] 2021-08-31 04:35:34,687 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "{'loss': 0.3006, 'learning_rate': 2.5e-05, 'epoch': 8.47}\n",
            "{'loss': 0.0854, 'learning_rate': 0.0, 'epoch': 16.95}\n",
            "100% 1000/1000 [12:16<00:00,  1.35it/s][INFO|trainer.py:1558] 2021-08-31 04:47:51,683 >> Saving model checkpoint to bert_output/checkpoint-1000\n",
            "[INFO|configuration_utils.py:314] 2021-08-31 04:47:52,612 >> Configuration saved in bert_output/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-08-31 04:48:01,996 >> Model weights saved in bert_output/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-08-31 04:48:02,548 >> tokenizer config file saved in bert_output/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-08-31 04:48:03,201 >> Special tokens file saved in bert_output/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|trainer.py:1129] 2021-08-31 04:48:32,346 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 777.6913, 'train_samples_per_second': 1.286, 'epoch': 16.95}\n",
            "100% 1000/1000 [12:57<00:00,  1.29it/s]\n",
            "[INFO|trainer.py:1558] 2021-08-31 04:48:32,731 >> Saving model checkpoint to bert_output/\n",
            "[INFO|configuration_utils.py:314] 2021-08-31 04:48:33,353 >> Configuration saved in bert_output/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-08-31 04:48:45,414 >> Model weights saved in bert_output/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-08-31 04:48:51,748 >> tokenizer config file saved in bert_output/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-08-31 04:48:52,287 >> Special tokens file saved in bert_output/special_tokens_map.json\n",
            "[INFO|trainer_pt_utils.py:656] 2021-08-31 04:48:53,661 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:53,661 >>   epoch                      =    16.95\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:53,661 >>   init_mem_cpu_alloc_delta   =      1MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:53,661 >>   init_mem_cpu_peaked_delta  =      0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:53,661 >>   init_mem_gpu_alloc_delta   =    422MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:53,661 >>   init_mem_gpu_peaked_delta  =      0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:53,662 >>   train_mem_cpu_alloc_delta  =      0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:53,662 >>   train_mem_cpu_peaked_delta =     94MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:53,662 >>   train_mem_gpu_alloc_delta  =   1297MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:53,662 >>   train_mem_gpu_peaked_delta =   1709MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:53,662 >>   train_runtime              = 777.6913\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:53,662 >>   train_samples              =      933\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:53,662 >>   train_samples_per_second   =    1.286\n",
            "08/31/2021 04:48:55 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:483] 2021-08-31 04:48:55,883 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence.\n",
            "[INFO|trainer.py:1775] 2021-08-31 04:48:55,887 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1776] 2021-08-31 04:48:55,887 >>   Num examples = 199\n",
            "[INFO|trainer.py:1777] 2021-08-31 04:48:55,887 >>   Batch size = 8\n",
            "100% 25/25 [00:03<00:00,  8.24it/s]\n",
            "[INFO|trainer_pt_utils.py:656] 2021-08-31 04:48:59,099 >> ***** eval metrics *****\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:59,099 >>   epoch                     =  16.95\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:59,099 >>   eval_accuracy             = 0.7538\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:59,099 >>   eval_loss                 = 1.5205\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:59,099 >>   eval_mem_cpu_alloc_delta  =    0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:59,099 >>   eval_mem_cpu_peaked_delta =    0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:59,099 >>   eval_mem_gpu_alloc_delta  =    0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:59,099 >>   eval_mem_gpu_peaked_delta =   33MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:59,099 >>   eval_runtime              = 3.0658\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:59,099 >>   eval_samples              =    199\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-31 04:48:59,100 >>   eval_samples_per_second   = 64.909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrI5QftFgccn"
      },
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class BertEvaluator:\n",
        "    def __init__(self):\n",
        "        # 事前学習済みのトークナイザとモデルをロード\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking', do_lower_case=False)\n",
        "        self.model = BertForSequenceClassification.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking', num_labels=2)\n",
        "        \n",
        "        # Google Colabでファインチューニングしたモデルをロード\n",
        "        self.model.load_state_dict(torch.load(\"bert_output/pytorch_model.bin\", map_location=\"cpu\"))\n",
        "        self.model.to(device)\n",
        "\n",
        "    def evaluate(self, user_input, candidate):\n",
        "        with torch.no_grad():\n",
        "            # 発話のペアを特徴ベクトルに変換\n",
        "            tokenized = self.tokenizer([[user_input, candidate]], return_tensors=\"pt\")\n",
        "            input_ids = tokenized[\"input_ids\"].to(device)\n",
        "            token_type_ids = tokenized[\"token_type_ids\"].to(device)\n",
        "\n",
        "            # ファインチューニング済みのBERTを用いて特徴ベクトルから2文のスコアを計算\n",
        "            result = self.model.forward(input_ids, token_type_ids=token_type_ids)\n",
        "            # softmax関数によりスコアを正規化\n",
        "            result = F.softmax(result[0], dim=1).cpu().numpy().tolist()\n",
        "\n",
        "            # 結果を返す．\n",
        "            return result[0][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPtLDJ9yiijs",
        "outputId": "4472dab8-eaf1-4efb-cf25-1209a6707bf5"
      },
      "source": [
        "be = BertEvaluator()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLP4liAqils7",
        "outputId": "ebecbc4e-32d7-412b-8164-830bc1d99c52"
      },
      "source": [
        "print(be.evaluate(\"お腹すいた\",\"なにか食べる？\"))\n",
        "print(be.evaluate(\"ありがとう\",\"どういたしまして\"))\n",
        "print(be.evaluate(\"ごめん\",\"許すよ\"))\n",
        "print(be.evaluate(\"お腹すいた\",\"ありがとう\"))\n",
        "print(be.evaluate(\"ありがとう\",\"君の名前は？\"))\n",
        "print(be.evaluate(\"ごめん\",\"明日の予定は？\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9214066863059998\n",
            "0.8842424154281616\n",
            "0.7122802138328552\n",
            "0.0017607016488909721\n",
            "0.00084912427701056\n",
            "0.00014908448792994022\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxXvQbK9gisG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944
        },
        "outputId": "bd30a23d-0630-43d9-9fe3-42af7a8534aa"
      },
      "source": [
        "from transformers import T5Tokenizer, AutoModelForCausalLM\n",
        "import re\n",
        "\n",
        "#文章をいくつ推論するか\n",
        "RETURN_NUM = 8\n",
        "#候補文をいくつ表示するか(RETURN_NUMより小さい値にしてください)\n",
        "OPTION_NUM = 4\n",
        "\n",
        "# トークナイザーとモデルの準備\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"output/\")\n",
        "#BERTを使った評価用クラスの準備\n",
        "be = BertEvaluator()\n",
        "\n",
        "u = \"\"\n",
        "next_sentence = input(\"\\n>\")\n",
        "log = []\n",
        "log.append(next_sentence)\n",
        "\n",
        "while(True):  \n",
        "  if \"exit\" == u:\n",
        "    break\n",
        "  if \"back\" == u:\n",
        "    log.pop()\n",
        "    next_sentence = re.split(\"(?<=。)\",log[-1])[0]\n",
        "    print(\"入力文：\")\n",
        "    print(next_sentence)\n",
        "  if \"log\" == u:\n",
        "    print(\"ログ：\")\n",
        "    print(\"\\n\")\n",
        "    print(\"\\n\".join(log))\n",
        "    print(\"\\n\")\n",
        "    print(\"入力文：\")\n",
        "    print(next_sentence)\n",
        "  if not \"。\" in next_sentence:\n",
        "    next_sentence = next_sentence + \"。\" \n",
        "  # 推論\n",
        "  encoded = tokenizer.encode(next_sentence, return_tensors=\"pt\")\n",
        "  output = model.generate(encoded, do_sample=True, max_length=100, num_return_sequences=RETURN_NUM)\n",
        "\n",
        "  sequence_dict = {}\n",
        "  for sequence in tokenizer.batch_decode(output):\n",
        "    sequence = sequence.replace('</s>', '')\n",
        "    sentence_list = re.split(\"(?<=。)\",sequence)[:-1]\n",
        "    sequence = \"\".join(sentence_list)\n",
        "    score = be.evaluate(\"\".join(log[-1]), sequence[len(next_sentence):])\n",
        "    sequence_dict[sequence] = score\n",
        "  sequence_dict = sorted(sequence_dict.items(), key=lambda x:x[1], reverse=True)\n",
        "  for i in range(OPTION_NUM):\n",
        "    print(\"[\", i,\"]\", sequence_dict[i][0])\n",
        "  \n",
        "  u = input(\"\\n>\")\n",
        "  if u.isdecimal():\n",
        "    choice_sequence = sequence_dict[int(u)][0]\n",
        "    log.append(choice_sequence[len(next_sentence):])\n",
        "    next_sentence = re.split(\"(?<=。)\", choice_sequence)[-2]\n",
        "    print(\"入力文：\")\n",
        "    print(next_sentence)\n",
        "\n",
        "  # 1. \">\"の右の入力欄に最初の一文を入力します\n",
        "  # 2. 入力文に続く[0]～[3]までの候補文が表示されます\n",
        "  # 3. 表示された候補の左の数字を\">\"の右の入力欄に入力することでその候補の最後の文が次の入力文になります\n",
        "  # 4. \"log\"と入力するとこれまでの文章が続けて表示されます\n",
        "  # 5. \"back\"と入力すると入力が一つ前まで戻ります\n",
        "  # 6. \"exit\"と入力すると終了します"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">おはよう\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[ 0 ] おはよう。 今日は大掃除だ...。 とんでもないことになっている!!やらないと... おはようございます!! 一年ぶりのブログ更新です。 皆さん元気にしてましたか!? 僕は元気です。 「... こんにちは。 今日は朝から雨が降ってます。 なんだか気分上がらない。\n",
            "[ 1 ] おはよう。 私は午前9時半に会場に着く。まだまだ暗い。どうすればいいだろうと悩みながら、私はそのことを考えていた。しかし、ふと、ふと思い出し、私は少し肩の力を抜いてみることにした。今、この記事を書いているところなのである。いや、別に迷っているわけではないのである。 なぜ......。 なぜ......。 なぜそんなことを思ったのか。\n",
            "[ 2 ] おはよう。 朝起きたらもう8時過ぎです! 今日は天気がずっとよく、とってもすてきな朝ごはんになるそうですよ。 今日も寒い一日になりそうですよ。\n",
            "[ 3 ] おはよう。 今日はいい天気になりました。 外はこんなに暗くありませんが。 そうです。 この前見た番組は、いい天気になると必ずやってくる、 太陽フレアと言うやつです。 何と言うか、地球へ太陽の光が届き、 その光が、太陽のまわりを回らなくなる状態のことをいいます。 その現象を、私たちはこう呼びます。\n",
            "\n",
            ">0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "入力文：\n",
            " なんだか気分上がらない。\n",
            "[ 0 ] なんだか気分上がらない。 ・。・。・。・。・・あ、そうか。それはちょっとやだな。でも、別にいいって言われても。\n",
            "[ 1 ] なんだか気分上がらない。 そうか、この雨は今日に降るから、いつもこうならんとあらば、おっかない! そうか! 私がせっせと布団と毛布と扇風機とを片付けないと、一日が終わらない! 「じゃ、やろうと」と言いたくなるが、それはまたいつものことだ。\n",
            "[ 2 ] なんだか気分上がらない。 ・・・というわけで、ちょっと早めに仕事を切り上げて、 ここはさすが、お客さまが待つ「the king brothers」だった。\n",
            "[ 3 ] なんだか気分上がらない。 ・・・・・・なんだか昨日のことのようです。 昨晩はいつものようにお酒を飲みながら、「ブログを書こう」なんて思っていたのですが、いつものように寝過ごしてしまして、気がつけば8時を回っていたのです。まあ、遅刻はいやだけど、遅くなってきたので、あわてて原稿を書き上げました。\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \"\"\"\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-aff54c1da7e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdecimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mchoice_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}